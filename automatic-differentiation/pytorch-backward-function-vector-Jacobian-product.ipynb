{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `PyTorch`'s `backward()` function and the vector-Jacobian product\n",
    "The  goal of this notebook is to understand how the `PyTorch` `backward()` function performs (automatic) differentiation of functions.\n",
    "\n",
    "# TLDR\n",
    "The important point is as follows:\n",
    "> `PyTorch`'s `backward()` function computes vector-Jacobian products.  By choosing the appropriate vector, which corresponds to the `gradient` argument in `backward()`, one can compute derivatives and gradients of functions.\n",
    "\n",
    "The rest of this notebook is structured as follows:\n",
    "- examples of usage without explanation;\n",
    "- correspondence between mathematical differentiation and `PyTorch`'s `backward()` function; and  \n",
    "- examples revisited, with justifications.\n",
    "\n",
    "# References\n",
    "- [The gradient argument in pytorch's backward function explained by examples](https://medium.com/@zhang_yang/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29) (Zhang Yang on Medium)\n",
    "- [Computing gradients with backpropagation](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/backprop.pdf) (Ryan Adams, Princeton)\n",
    "\n",
    "---\n",
    "tags: pytorch, tutorial, backward, automatic differentiation, vector-Jacobian product, mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the derivative of $f(x)\\in \\mathbb R$ evaluated at a single $x\\in\\mathbb R$\n",
    "Evaluate the derivative of $f(x)=x^2$ at $x=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the derivative of $f(x)\\in\\mathbb R$ evaluated at $x_i\\in\\mathbb R$, $i=1, \\dots, n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(-1, 1, 5, requires_grad=True)\n",
    "y = x**2\n",
    "v = torch.ones_like(y)\n",
    "y.backward(v)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the gradient of $f(x)\\in\\mathbb R$ evaluated at a single $x\\in\\mathbb R^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6., 20.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([-1., 2.], requires_grad=True)\n",
    "w = torch.tensor([3., 5.])\n",
    "y = (x*x*w).sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the gradient of $f(x)\\in\\mathbb R$  at $x_i\\in\\mathbb R^n$, $i=1\\,, \\dots\\,, n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  1.],\n",
       "        [-1.,  1.],\n",
       "        [-1.,  1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6, dtype=float).view(3, 2)\n",
    "x.requires_grad = True\n",
    "w = torch.tensor([-1, 1])\n",
    "y = (x*w).sum(1)\n",
    "y.backward(torch.ones_like(y))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the derivative of $f(x)\\in \\mathbb R^m$ evaluated at a single $x\\in\\mathbb R$\n",
    "Evaluate the derivative of $f(x)= (-x^3, 5x)$ evaluated at $x=1$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y(1.0)    = [-1.0, 5.0]\n",
      "y_1'(1.0) = -3.0\n",
      "y_2'(1.0) =  5.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = torch.stack([-x**3, 5*x]).view(1, -1)\n",
    "print(f\"y({x.data.item()})    = {list(y.data.numpy()[0,:])}\")\n",
    "\n",
    "v1 = torch.tensor([[1., 0.]])\n",
    "y.backward(v1, retain_graph=True)\n",
    "print(f\"y_1'({x.data.item()}) = {x.grad.data.item():>4}\")\n",
    "\n",
    "x.grad.zero_()\n",
    "\n",
    "v2 = torch.tensor([[0., 1.]])\n",
    "y.backward(v2, retain_graph=True)\n",
    "print(f\"y_2'({x.data.item()}) = {x.grad.data.item():>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The correspondence between mathematical differentiation and `PyTorch`'s `backward()` function\n",
    "It is useful to draw a careful analogy between terminology which is conventional in mathematics and that used in `PyTorch`.\n",
    "\n",
    "## Warning: \"dimension\"\n",
    "The notion of dimension in mathematics is not the same as that in `PyTorch`.\n",
    "\n",
    "## Preliminaries\n",
    "### Mathematical notation\n",
    "\n",
    "- The set of real values is denoted by $\\mathbb R$.  \n",
    "- The set of **vectors of dimension $n$**, such as $x=(x_1, \\dots, x_n)$, is denoted $\\mathbb R^n$.  Vectors correspond to `0`-dimensional `tensor`s.\n",
    "- The set of **$m\\times n$ matrices**, such as\n",
    "$\\begin{bmatrix}\n",
    "    x_{1,1}&\\dots&x_{1,n}\\\\\n",
    "    \\vdots&\\ddots&\\vdots\\\\\n",
    "    x_{m,1}&\\dots&x_{m,n}\n",
    "\\end{bmatrix}$,\n",
    "is denoted\n",
    "$\\mathbb R^{m\\times n}$.\n",
    "- The set of **row vectors**, i.e. are single-row matrices, is thus $\\mathbb R^{1,n}$.\n",
    "- The set of **column vectors**, i.e. single-column matrices, is thus $\\mathbb R^{m,1}$.  \n",
    "\n",
    "The three sets $\\mathbb R^n$, $\\mathbb R^{1,n}$, and $\\mathbb R^{n,1}$, are distinct although it is common to identify them.\n",
    "\n",
    "Observe that vectors and row vectors can be distinguished by their notation: the vector $(x_1,\\dots, x_n)\\in\\mathbb R^n$ is not the row vector $\\left[x_1,\\dots, x_n\\right]\\in\\mathbb R^{1,n}\n",
    "$.\n",
    "\n",
    "\n",
    "### `PyTorch` `tensor`s\n",
    "- By `tensor`, we mean a `PyTorch` object of the class `torch.tensor`.  Its shape `torch.Size([m,n])` will simply be denoted `[m,n]`.\n",
    "- Real scalars correspond to `0`-dimensional `tensor`s.\n",
    "- Vectors correspond to `1`-dimensional `tensor`s.\n",
    "- Row vectors correspond to `2`-dimensional `tensor`s with shape `[1,2]`.\n",
    "- Column vectors correspond to `2`-dimensional `tensor`s with shape `[2,1]`.\n",
    "\n",
    "\n",
    "### Prerequisites on differentiation\n",
    "\n",
    "We do not recall the definitions for:\n",
    "- the **derivative** $f'(x)$ of a scalar, uni-variate function $f(x)$, $x\\in\\mathbb R$;\n",
    "- the **partial derivatives** $\\frac{\\partial f}{\\partial x_i}$, $i=1, \\dots, n$, of a scalar, multi-variate function $f(x)\\in \\mathbb R$, $x\\in\\mathbb R^n$;\n",
    "- **Einstein's convention** of  summing over repeated indices.\n",
    "\n",
    "In particular, when we speak of the derivative, it is implicit that the function is univariate, i.e. it depends on a single (real) variable $x$.\n",
    "\n",
    "### Derivatives of vector-valued functions $f(x)\\in\\mathbb R^m$, $x\\in\\mathbb R$\n",
    "The **derivative** of a vector-valued function $f(x)=(f_1(x), \\dots, f_m(x))\\in\\mathbb R^m$ is the column vector of the derivatives of the component functions:\n",
    "$$\\begin{bmatrix}f_1'(x)\\\\\\vdots\\\\f_m'(x)\\end{bmatrix}\\in\\mathbb R^{m\\times 1}\\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients of scalar-valued functions $f(x)\\in\\mathbb R$, $x\\in\\mathbb R^n$\n",
    "The **gradient** of a scalar-valued function $f(x)\\in\\mathbb R$, $x\\in\\mathbb R^n$, is the *row* vector of its partial derivatives:\n",
    "$$\\nabla f(x) = \\left[\\frac{\\partial f}{\\partial x_1}(x)\\,,\\dots\\,,\\frac{\\partial f}{\\partial x_n}(x)\\right]\\in\\mathbb R^{1\\times n}\\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobians\n",
    "The **Jacobian** of a vector-valued function $f(x)=(f_1(x)\\,, \\dots\\,, f_m(x))\\in\\mathbb R^m$, $x\\in\\mathbb R^n$, is the vertical concatenation of the gradients of the component functions $f_1, \\dots, f_m$:\n",
    "$$J_f(x)=\\begin{bmatrix}\n",
    "    \\frac{\\partial f_1}{\\partial x_1}(x)&\\dots&\\frac{\\partial f_1}{\\partial x_n}(x)\\\\\n",
    "    \\vdots&\\ddots&\\vdots\\\\\n",
    "    \\frac{\\partial f_m}{\\partial x_1}(x)&\\dots&\\frac{\\partial f_m}{\\partial x_n}(x)\n",
    "\\end{bmatrix}\\in\\mathbb R^{m\\times n}\\,.\n",
    "$$\n",
    "#### The case $m=1$\n",
    "In case $m=1$, the Jacobian agrees with the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-Jacobian products\n",
    "Given a row vector $v\\in\\mathbb R^{1\\times m}$, the **vector-Jacobian product** is the row vector\n",
    "$$v J_f(x) = \\left[\n",
    "v_j\\frac{\\partial f_j}{\\partial x_1}(x)\\,, \\dots\\,, v_j\\frac{\\partial f_j}{\\partial x_n}(x)\n",
    "\\right]\\in\\mathbb R^{1\\times n\n",
    "}\\,,\n",
    "$$\n",
    "where we have used the convention of summing over repeated indices.\n",
    "\n",
    "\n",
    "In particular, if $v$ is the gradient of a scalar-valued function $z=l(y)$ evaluated at $f(x)$, i.e. $v = \\nabla l(y)$ where $y=f(x)$, then\n",
    "$$v J_f(x) = \\left[\\frac{\\partial l}{\\partial y_j}(y)\\frac{\\partial f_j}{\\partial x_1}(x)\\,, \\dots\\,,\n",
    "\\frac{\\partial l}{\\partial y_j}(y)\\frac{\\partial f_j}{\\partial x_m}(x)\\right]\n",
    "\\,=\\,\\nabla (l\\circ f)(x)\\in\\mathbb R^{1\\times n}\\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `PyTorch`'s `backward()` function\n",
    "With `x` denoting the input `tensor` and `y` denoting the output `tensor`, we will call the `backward()` method on `y` to compute the (partial) derivatives at `x`.  These derivatives will be collected (\"accumulated\") in the attribute `x.grad`.\n",
    "\n",
    "`PyTorch`'s `backward()` function, on the other hand, computes vector-Jacobian products.  To this end, we need to determine what corresponds to the row vector $v$ above.\n",
    "\n",
    "### The `gradient` argument in `backward()`\n",
    "The `gradient` argument in the function `backward()` corresponds to the row vector $v$ in the vector-Jacobian product introduced above.\n",
    "\n",
    "`PyTorch`'s `backward()` function takes an argument `gradient` which corresponds to the row vector $v$ above.  It takes a default value of `torch.tensor(1.)`.\n",
    "\n",
    "If `y` is `0`-dimensional or `1`-dimensional with shape `[1,1]`, `gradient` can be `0`-dimensional or `1`-dimensional with shape `[1,1]`.  In particular, if `y` corresponds to the output of a scalar function $f(x)$, regardless of whether uni-variate or multi-variate, the `gradient` argument can be ignored.\n",
    "\n",
    "Otherwise `v` must have same shape as `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the derivative of $f(x)\\in\\mathbb R$ at a single $x\\in\\mathbb R$\n",
    "The notation implies that the function is uni-variate (i.e. it depends on a single variable) and is scalar (\n",
    "i.e. it returns a single value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1., requires_grad=True),\n",
       " tensor(1., grad_fn=<PowBackward0>),\n",
       " tensor(2.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "x, y, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the `gradient` and assigning it a value of `1.` will return the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1., requires_grad=True),\n",
       " tensor(1., grad_fn=<PowBackward0>),\n",
       " tensor(2.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = x**2\n",
    "v = torch.tensor(1.)\n",
    "y.backward(v)\n",
    "x, y, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `x` or `v` can be replaced with `torch.tensor([1.])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the derivative of $f(x)\\in\\mathbb R$ at $x_i\\in\\mathbb R$, $i=1, \\dots, n$\n",
    "To evaluate the derivative of a scalar, univariate function $f(x)\\in\\mathbb R$ at multiple values $x_1, \\dots, x_n\\in\\mathbb R$, we create a new, multi-variate *and* vector-valued function of $x = (x_1,\\dots, x_n)\\in\\mathbb R^n$:\n",
    "$$F(x)=\\left[f(x_1)\\,,\\dots\\,, f(x_n)\\right]\\,.$$\n",
    "Thus, its Jacobian is\n",
    "$$J_F(x)=\\begin{bmatrix}\n",
    "f'(x_1)&&&&\\\\\n",
    "&\\ddots&&&\\\\\n",
    "&&f'(x_j)&&\\\\\n",
    "&&&\\ddots&\\\\\n",
    "&&&&f'(x_n)\\end{bmatrix}\n",
    "$$\n",
    "where the off-diagonal terms are all $0$.\n",
    "Thus, with the row vector of $1$'s, $v=\\mathbb 1_{1\\times n}\\in\\mathbb R^{1\\times n}$, we obtain the gradient of $f$ evaluated at $x_1\\,, \\dots\\,, x_n$:\n",
    "$$\\left[f'(x_1)\\,,\\dots\\,, f'(x_j)\\,, \\dots\\,, f'(x_n)\\right] \n",
    "=\\left[1\\,,\\dots\\,,1\\right]\n",
    "\\begin{bmatrix}\n",
    "f'(x_1)&&&&\\\\\n",
    "&\\ddots&&&\\\\\n",
    "&&f'(x_j)&&\\\\\n",
    "&&&\\ddots&\\\\\n",
    "&&&&f'(x_n)\\end{bmatrix}\n",
    "= \\mathbb 1_{\\mathbb R^{1\\times n}}J_F(x)\\,.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0000, -0.5000,  0.0000,  0.5000,  1.0000], requires_grad=True),\n",
       " tensor([1.0000, 0.2500, 0.0000, 0.2500, 1.0000], grad_fn=<PowBackward0>),\n",
       " tensor([-2., -1.,  0.,  1.,  2.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(-1, 1, 5, requires_grad=True)\n",
    "y = x**2\n",
    "v = torch.ones_like(y)\n",
    "y.backward(v)\n",
    "x, y, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "- `y` corresponds to $F(x)$;\n",
    "- `v` must have same shape as `y`;\n",
    "- `x` and `x.grad` have same shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the gradient of $f(x)\\in\\mathbb R$ at a single $x\\in\\mathbb R^n$\n",
    "Since the function is scalar, the `gradient` argument can be ignored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.,  2.], requires_grad=True),\n",
       " tensor(23., grad_fn=<SumBackward0>),\n",
       " tensor([-6., 20.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([-1., 2.], requires_grad=True)\n",
    "w = torch.tensor([3., 5.])\n",
    "y = (x*x*w).sum()\n",
    "y.backward()\n",
    "x, y, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the gradient of $f(x)\\in\\mathbb R$ at $x_i\\in\\mathbb R^n$, $i=1, \\dots, n$\n",
    "\n",
    "Here, `y` corresponds to $F(x)=(f(x_1), \\dots, f(x_n))\\in\\mathbb R^m$, where $m=3$ and $n=2$, and noting that `x` has shape `[m,n]`.  In turn, the `gradient` argument corresponds to $\\left[1,\\dots,1\\right]=\\mathbb 1_{\\mathbb R^{1,n}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1.],\n",
       "         [2., 3.],\n",
       "         [4., 5.]], dtype=torch.float64, requires_grad=True),\n",
       " tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<SumBackward1>),\n",
       " tensor([[-1.,  1.],\n",
       "         [-1.,  1.],\n",
       "         [-1.,  1.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6, dtype=float).view(3, 2)\n",
    "x.requires_grad = True\n",
    "w = torch.tensor([-1, 1])\n",
    "y = (x*w).sum(1)\n",
    "y.backward(torch.ones_like(y))\n",
    "x, y, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the derivative of $f(x)\\in\\mathbb R^m$ at a single $x\\in\\mathbb R$\n",
    "There are two complications.\n",
    "1. The derivative for each component function $f_1, \\dots, f_m$ of $f(x)$ is calculated one at a time, because `backward()` implements **reverse** differentiation.  In order to this, the flag `retain_graph` must be set to `True`. \n",
    "1. Since derivatives are accumulated in `x`, its `grad` attribute must be set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y(1.0)  = [-1.0, 5.0]\n",
      "y_1'(1) = -3.0\n",
      "y_2'(1) =  5.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = torch.stack([-x**3, 5*x]).view(1, -1)\n",
    "print(f\"y({x.data.item()})  = {list(y.data.numpy()[0,:])}\")\n",
    "\n",
    "v1 = torch.tensor([[1., 0.]])\n",
    "y.backward(v1, retain_graph=True)\n",
    "print(f\"y_1'(1) = {x.grad.data.item():>4}\")\n",
    "\n",
    "x.grad.zero_()\n",
    "\n",
    "v2 = torch.tensor([[0., 1.]])\n",
    "y.backward(v2, retain_graph=True)\n",
    "print(f\"y_2'(1) = {x.grad.data.item():>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further cases: input `tensor`s of higher dimensions\n",
    "\n",
    "The above also works if $x$ is a matrix, or more generally a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.],\n",
       "         [3., 4.]], requires_grad=True),\n",
       " tensor(10., grad_fn=<SumBackward0>),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "w = torch.ones_like(x)\n",
    "y = (x*w).sum()\n",
    "y.backward()\n",
    "x, y, x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 9]),\n",
       " torch.Size([9, 9]),\n",
       " torch.Size([9, 9]),\n",
       " torch.Size([9, 9]),\n",
       " torch.Size([81, 2]),\n",
       " torch.Size([81, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min, x_max, y_min, y_max = -4, 4., -4., 4.\n",
    "\n",
    "linear = nn.Linear(2, 1)\n",
    "linear.bias.data = torch.tensor([0.])\n",
    "\n",
    "N = 9\n",
    "Y, X = np.mgrid[y_min:y_max:N*1j, x_min:x_max:N*1j]\n",
    "X = torch.tensor(X, dtype=torch.float32, requires_grad=True)\n",
    "Y = torch.tensor(Y, dtype=torch.float32, requires_grad=True)\n",
    "XY = torch.stack([X, Y], -1).reshape(N*N,2)\n",
    "\n",
    "Z = linear(XY).sigmoid()\n",
    "Z.backward(torch.ones_like(Z))\n",
    "gradf_X, gradf_Y = X.grad, Y.grad\n",
    "\n",
    "X.shape, gradf_X.shape, Y.shape, gradf_Y.shape, XY.shape, Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
